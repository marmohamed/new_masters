{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/injy/mariam_workspace/env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/injy/mariam_workspace/env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/injy/mariam_workspace/env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/injy/mariam_workspace/env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/injy/mariam_workspace/env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/injy/mariam_workspace/env/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/injy/mariam_workspace/env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/injy/mariam_workspace/env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/injy/mariam_workspace/env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/injy/mariam_workspace/env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/injy/mariam_workspace/env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/injy/mariam_workspace/env/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.segmentation_dataset_loader import *\n",
    "from data.detection_dataset_loader import *\n",
    "from model import *\n",
    "from Trainer import *\n",
    "from evaluation.evaluate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.postprocessing.nms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_feed_dict(model, dataset, train_fusion_rgb, train_fusion_fv_lidar, anchor_values, use_nms):\n",
    "        # camera_tensor, lidar_tensor, fv_velo_tensor, label_tensor, Tr_velo_to_cam, R0_rect, P3, shift_h, shift_w = sess.run(dataset)\n",
    "        data = dataset.get_next(batch_size=1)\n",
    "\n",
    "#         for i in range(len(data)):\n",
    "#             data[i] = np.expand_dims(data[i], axis=0)\n",
    "        camera_tensor, lidar_tensor, label_tensor, Tr_velo_to_cam, R0_rect, P3, shift_h, shift_w = data\n",
    "#         print(np.max(camera_tensor))\n",
    "        d = {model.train_inputs_rgb: camera_tensor,\n",
    "                model.train_inputs_lidar: lidar_tensor,\n",
    "#                 model.Tr_velo_to_cam: Tr_velo_to_cam,\n",
    "#                 model.R0_rect: R0_rect,\n",
    "#                 model.P3: P3,\n",
    "#                 model.shift_h: shift_h,\n",
    "#                 model.shift_w: shift_w,\n",
    "#                 model.anchors: anchor_values,\n",
    "#                 model.use_nms: use_nms,\n",
    "                model.train_fusion_rgb: train_fusion_rgb,\n",
    "                model.is_training: False}\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return (1 / (1 + np.exp(-x.astype(np.float128)))).astype(np.float32)\n",
    "\n",
    "def convert_prediction_into_real_values(label_tensor, \n",
    "            anchors=np.array([3.9, 1.6, 1.5]), \n",
    "            input_size=(512, 448), output_size=(128, 112), is_label=False, th=0.5):\n",
    "\n",
    "    ratio = input_size[0] // output_size[0]\n",
    "    result = []\n",
    "    ones_index = np.where(sigmoid(label_tensor[:, :, :, -1])>=th)\n",
    "    if len(ones_index) > 0 and len(ones_index[0]) > 0:\n",
    "        for i in range(0, len(ones_index[0]), 1):\n",
    "            x = ones_index[0][i]\n",
    "            y = ones_index[1][i]\n",
    "            \n",
    "            out = np.copy(label_tensor[ones_index[0][i], ones_index[1][i], ones_index[2][i], :])\n",
    "            anchor = np.array([x+0.5, y+0.5, 0.5, anchors[0], anchors[1], anchors[2]])\n",
    "#             if not is_label:\n",
    "#               out[:3] = sigmoid(out[:3])\n",
    "            out[:3] = np.tanh(out[:3])*0.5 * anchor[3:6] + anchor[:3]\n",
    "            \n",
    "            out[:2] = out[:2] * ratio\n",
    "            out[2] = out[2] * 40\n",
    "            \n",
    "            out[3:6] = np.exp(np.maximum(0, out[3:6])) * anchors\n",
    "            \n",
    "            k = ones_index[2][i]\n",
    "            if not is_label:\n",
    "              out[6] = sigmoid(out[6]) * np.pi/2 - np.pi/4\n",
    "            if k == 0 and out[6] < 0:\n",
    "                out[6] = out[6] + np.pi\n",
    "                \n",
    "            out[6] = out[6] + k * (np.pi/2)\n",
    "                        \n",
    "            result.append(out)\n",
    "            \n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_points(converted_points, calib_path, \n",
    "                x_range=(0, 71), y_range=(-40, 40), z_range=(-3.0, 1), \n",
    "                size=(512, 448, 40), th=0.5):\n",
    "    all_result = []\n",
    "    for converted_points_ in converted_points:\n",
    "        if sigmoid(converted_points_[-1]) >= th:\n",
    "#              and sigmoid(converted_points_[8]) < th2\n",
    "            result = [0] * 16\n",
    "            result[0] = 'Car'\n",
    "            result[1] = -1\n",
    "            result[2] = -1\n",
    "            result[3] = -10\n",
    "            result[8] = converted_points_[5]\n",
    "            result[9] = converted_points_[4]\n",
    "            result[10] = converted_points_[3]\n",
    "            result[14] = converted_points_[6]\n",
    "            result[15] = sigmoid(converted_points_[-1])\n",
    "\n",
    "            calib_data = read_calib(calib_path)\n",
    "\n",
    "            # x_range=(0, 70)\n",
    "            # y_range=(-40, 40)\n",
    "            # z_range=(-2.5, 1)\n",
    "\n",
    "            x_size = (x_range[1] - x_range[0])\n",
    "            y_size = (y_range[1] - y_range[0])\n",
    "            z_size = (z_range[1] - z_range[0])\n",
    "\n",
    "            x_fac = (size[0]-1) / x_size\n",
    "            y_fac = (size[1]-1) / y_size\n",
    "            z_fac = (size[2]-1) / z_size\n",
    "\n",
    "            x, y, z = -((converted_points_[:3] - size) / np.array([x_fac, y_fac, z_fac])) - np.array([0, -1*y_range[0], -1*z_range[0]]) \n",
    "            point = np.array([[x, y, z]])\n",
    "            box3d_pts_3d = point\n",
    "\n",
    "            pts_3d_ref = project_velo_to_ref(box3d_pts_3d, calib_data['Tr_velo_to_cam'].reshape((3, 4)))\n",
    "            pts_3d_ref = project_ref_to_rect(pts_3d_ref, calib_data['R0_rect'].reshape((3, 3)))[0]\n",
    "            for k in range(3):\n",
    "                result[11 + k] = pts_3d_ref[k]\n",
    "\n",
    "            imgbbox = ProjectTo2Dbbox(pts_3d_ref, converted_points_[5], converted_points_[4],\n",
    "                         converted_points_[3], converted_points_[6], calib_data['P2'].reshape((3, 4)))\n",
    "\n",
    "            result[4:8] = imgbbox\n",
    "            all_result.append(result)\n",
    "    return all_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label2(label_path, calib_path, shift_h, shift_w, x_range=(0, 71), y_range=(-40, 40), z_range=(-3.0, 1), \n",
    "                    size=(512, 448, 40), get_actual_dims=False, from_file=True, translate_x=0, translate_y=0, ang=0, get_neg=False):\n",
    "#     return __read_label(label_path, calib_path, shift_h, shift_w, x_range=x_range, y_range=y_range, z_range=z_range, \n",
    "#                     size=size, get_actual_dims=get_actual_dims, from_file=from_file, translate_x=translate_x, translate_y=translate_y, ang=ang, get_neg=get_neg)\n",
    "\n",
    "\n",
    "\n",
    "# def __read_label(label_path, calib_path, shift_h, shift_w, x_range=(0, 71), y_range=(-40, 40), z_range=(-3.0, 1), \n",
    "#                     size=(512, 448, 32), get_actual_dims=False, from_file=True, translate_x=0, translate_y=0, ang=0,\n",
    "#                     get_neg=False):\n",
    "    \"\"\"\n",
    "    the file format is as follows: \n",
    "    type, truncated, occluded, alpha, bbox_left, bbox_top, bbox_right, bbox_bottom,\n",
    "    dimensions_height, dimensions_width, dimensions_length, location_x, location_y, location_z,\n",
    "    rotation_y, score) \n",
    "    \"\"\"\n",
    "    if from_file:\n",
    "        lines = []\n",
    "        with open(label_path) as label_file:\n",
    "            lines = label_file.readlines()\n",
    "    else:\n",
    "        lines = label_path.split('\\n')\n",
    "    # filter car class\n",
    "    lines = list(map(lambda x: x.split(), lines))\n",
    "    if len(lines) > 0:\n",
    "        if get_neg:\n",
    "            lines = list(filter(lambda x: len(x) > 0 and ( x[0] not in ['Car', 'Van', 'Truck', 'Tram', 'DontCare']), lines))\n",
    "            if len(lines) > 0:\n",
    "                lines = lines[:1]\n",
    "        else:\n",
    "            lines = list(filter(lambda x: len(x) > 0 and ( x[0] in ['Car', 'Van', 'Truck', 'Tram']), lines))\n",
    "    \n",
    "    def get_parameter(index):\n",
    "        return list(map(lambda x: x[index], lines))\n",
    "    \n",
    "    classes = get_parameter(0)\n",
    "    dimension_height = np.array(get_parameter(8)).astype(float)\n",
    "    dimension_width = np.array(get_parameter(9)).astype(float)\n",
    "    dimension_length = np.array(get_parameter(10)).astype(float)\n",
    "    # TODO: take shift into consideration - URGENT\n",
    "    location_x = np.array(get_parameter(11)).astype(float)\n",
    "    location_y = np.array(get_parameter(12)).astype(float)\n",
    "    location_z = np.array(get_parameter(13)).astype(float)\n",
    "    angles = np.array(get_parameter(14)).astype(float)\n",
    "    \n",
    "    # print(len(classes))\n",
    "    calib_data = read_calib(calib_path)\n",
    "\n",
    "    locations = np.array([[location_x[i], location_y[i], location_z[i]] for i in range(len(classes))])\n",
    "    # print(locations)\n",
    "    if len(locations) > 0 and len(locations[0]) > 0:\n",
    "        locations = project_rect_to_velo(locations, calib_data['R0_rect'].reshape((3, 3)), calib_data['Tr_velo_to_cam'].reshape((3, 4)))\n",
    "    # print(z_range)\n",
    "    indxes = np.array(list(map(lambda point: (point[0] >= x_range[0]  and point[0] <= x_range[1])\n",
    "                                    and (point[1] >= y_range[0] and point[1] <= y_range[1])\n",
    "                                    and (point[2] >= z_range[0] and point[2] <= z_range[1]) , locations)))\n",
    "\n",
    "    locations = np.array(list(filter(lambda point: (point[0] >= x_range[0]  and point[0] <= x_range[1])\n",
    "                                    and (point[1] >= y_range[0] and point[1] <= y_range[1])\n",
    "                                    and (point[2] >= z_range[0] and point[2] <= z_range[1]) , locations)))\n",
    "\n",
    "    if len(locations) > 0:\n",
    "        locations[:, :2] = locations[:, :2] - np.array([translate_x, translate_y])\n",
    "\n",
    "    # print('.......')\n",
    "    \n",
    "\n",
    "    points = [project_point_from_camera_coor_to_velo_coor([location_x[i], location_y[i], location_z[i]], \n",
    "                                                        [dimension_height[i], dimension_width[i], dimension_length[i]],\n",
    "                                                        angles[i],\n",
    "                                                         calib_data)\n",
    "                for i in range(len(locations))]\n",
    "    \n",
    "    x_size = (x_range[1] - x_range[0])\n",
    "    y_size = (y_range[1] - y_range[0])\n",
    "    z_size = (z_range[1] - z_range[0])\n",
    "            \n",
    "    x_fac = (size[0]-1) / x_size\n",
    "    y_fac = (size[1]-1) / y_size\n",
    "    z_fac = (size[2]-1) / z_size\n",
    "    if get_actual_dims:\n",
    "        import math\n",
    "        for i in range(len(points)):\n",
    "            b = points[i]\n",
    "            x0 = b[0][0]\n",
    "            y0 = b[0][1]\n",
    "            x1 = b[1][0]\n",
    "            y1 = b[1][1]\n",
    "            x2 = b[2][0]\n",
    "            y2 = b[2][1]\n",
    "            u0 = -(x0) * x_fac + size[0]\n",
    "            v0 = -(y0 + 40) * y_fac + size[1]\n",
    "            u1 = -(x1) * x_fac + size[0]\n",
    "            v1 = -(y1 + 40) * y_fac + size[1]\n",
    "            u2 = -(x2) * x_fac + size[0]\n",
    "            v2 = -(y2 + 40) * y_fac + size[1]\n",
    "            # print(dimension_length[i])\n",
    "            dimension_length[i] = math.sqrt((v1-v2)**2 + (u1-u2)**2)\n",
    "            # print(dimension_length[i])\n",
    "            dimension_width[i] = math.sqrt((v1-v0)**2 + (u1-u0)**2)\n",
    "            # print(dimension_height[i])\n",
    "            dimension_height[i] = math.sqrt((-(b[0][2]+(-1*z_range[1]))*z_fac-(-b[4][2]+z_range[1])*z_fac)**2)\n",
    "            # print(dimension_height[i])\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "    x_range = (x_range[0] + translate_x, x_range[1] + translate_x)\n",
    "    y_range = (y_range[0] + translate_y, y_range[1] + translate_y)\n",
    "    output = [[-(locations[i][0]) * x_fac + size[0], -(locations[i][1] + -1*y_range[0]) * y_fac + size[1], -(locations[i][2] + -1*z_range[0]) * z_fac + size[2], \n",
    "                dimension_height[i], dimension_width[i], dimension_length[i], angles[i]] \n",
    "                for i in range(len(locations))]\n",
    "    # import math\n",
    "    if ang != 0:\n",
    "        for i in range(len(locations)):\n",
    "            w = size[0]\n",
    "            h = size[1]\n",
    "            output[i][0], output[i][1] = rotate2((w//2, h//2), (output[i][0], output[i][1]), ang / 57.2958)\n",
    "            output[i][6] = output[i][6] - ang / 57.2958\n",
    "\n",
    "    output = np.array(output)\n",
    "    if from_file:\n",
    "        return points, output, calib_data['Tr_velo_to_cam'], calib_data['R0_rect'], calib_data['P2']\n",
    "    else:\n",
    "        return output, indxes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label2(label_path, calib_path, shift_h, shift_w, x_range=(0, 71), y_range=(-40, 40), z_range=(-3.0, 1), \n",
    "                    size=(512, 448, 40), get_actual_dims=False, from_file=True, translate_x=0, translate_y=0, translate_z=0, ang=0, get_neg=False):\n",
    "\n",
    "    \"\"\"\n",
    "    the file format is as follows: \n",
    "    type, truncated, occluded, alpha, bbox_left, bbox_top, bbox_right, bbox_bottom,\n",
    "    dimensions_height, dimensions_width, dimensions_length, location_x, location_y, location_z,\n",
    "    rotation_y, score) \n",
    "    \"\"\"\n",
    "    if from_file:\n",
    "        lines = []\n",
    "        with open(label_path) as label_file:\n",
    "            lines = label_file.readlines()\n",
    "    else:\n",
    "        lines = label_path.split('\\n')\n",
    "#     print(len(lines))\n",
    "    # filter car class\n",
    "    lines = list(map(lambda x: x.split(), lines))\n",
    "    if len(lines) > 0:\n",
    "        if get_neg:\n",
    "            lines = list(filter(lambda x: len(x) > 0 and ( x[0] not in ['Car', 'Van', 'Truck', 'Tram', 'DontCare']), lines))\n",
    "            if len(lines) > 0:\n",
    "                lines = lines[:1]\n",
    "        else:\n",
    "            lines = list(filter(lambda x: len(x) > 0 and ( x[0] in ['Car', 'Van', 'Truck', 'Tram']), lines))\n",
    "    \n",
    "    def get_parameter(index):\n",
    "        return list(map(lambda x: x[index], lines))\n",
    "    \n",
    "    classes = np.array(get_parameter(0))\n",
    "    dimension_height = np.array(get_parameter(8)).astype(float)\n",
    "    dimension_width = np.array(get_parameter(9)).astype(float)\n",
    "    dimension_length = np.array(get_parameter(10)).astype(float)\n",
    "    # TODO: take shift into consideration - URGENT\n",
    "    location_x = np.array(get_parameter(11)).astype(float)\n",
    "    location_y = np.array(get_parameter(12)).astype(float)\n",
    "    location_z = np.array(get_parameter(13)).astype(float)\n",
    "    angles = np.array(get_parameter(14)).astype(float)\n",
    "    directions = np.array(angles>= 0).astype(float)\n",
    "    \n",
    "    # print(len(classes))\n",
    "    calib_data = read_calib(calib_path)\n",
    "\n",
    "    locations = np.array([[location_x[i], location_y[i], location_z[i]] for i in range(len(classes))])\n",
    "\n",
    "    if len(locations) > 0 and len(locations[0]) > 0:\n",
    "        locations = project_rect_to_velo(locations, calib_data['R0_rect'].reshape((3, 3)), calib_data['Tr_velo_to_cam'].reshape((3, 4)))\n",
    "#     print(len(locations))\n",
    "    # print(z_range)\n",
    "\n",
    "    indx = []\n",
    "    i = 0\n",
    "    for point in locations:\n",
    "        if (point[0] >= x_range[0]  and point[0] <= x_range[1])\\\n",
    "            and (point[1] >= y_range[0] and point[1] <= y_range[1])\\\n",
    "            and (point[2] >= z_range[0] and point[2] <= z_range[1]):\n",
    "            indx.append(i)\n",
    "        i += 1\n",
    "\n",
    "    indxes = np.array(list(map(lambda point: (point[0] >= x_range[0]  and point[0] <= x_range[1])\n",
    "                                    and (point[1] >= y_range[0] and point[1] <= y_range[1])\n",
    "                                    and (point[2] >= z_range[0] and point[2] <= z_range[1]) , locations)))\n",
    "    locations = np.array(list(filter(lambda point: (point[0] >= x_range[0]  and point[0] <= x_range[1])\n",
    "                                    and (point[1] >= y_range[0] and point[1] <= y_range[1])\n",
    "                                    and (point[2] >= z_range[0] and point[2] <= z_range[1]) , locations)))\n",
    "\n",
    "    if len(indx) > 0:\n",
    "        dimension_height = dimension_height[indx]\n",
    "        dimension_width = dimension_width[indx]\n",
    "        dimension_length = dimension_length[indx]\n",
    "        location_x = location_x[indx]\n",
    "        location_y = location_y[indx]\n",
    "        location_z = location_z[indx]\n",
    "        angles = angles[indx]\n",
    "        classes = classes[indx]\n",
    "        directions = directions[indx]\n",
    "\n",
    "    if len(locations) > 0:\n",
    "        locations[:, :3] = locations[:, :3] - np.array([translate_x, translate_y, -translate_z])\n",
    "\n",
    "    # print('.......')\n",
    "    # print(len(locations))\n",
    "\n",
    "    points = [project_point_from_camera_coor_to_velo_coor([location_x[i], location_y[i], location_z[i]], \n",
    "                                                        [dimension_height[i], dimension_width[i], dimension_length[i]],\n",
    "                                                        angles[i],\n",
    "                                                         calib_data)\n",
    "                for i in range(len(locations))]\n",
    "    \n",
    "    x_size = (x_range[1] - x_range[0])\n",
    "    y_size = (y_range[1] - y_range[0])\n",
    "    z_size = (z_range[1] - z_range[0])\n",
    "            \n",
    "    x_fac = (size[0]-1) / x_size\n",
    "    y_fac = (size[1]-1) / y_size\n",
    "    z_fac = (size[2]-1) / z_size\n",
    "    if get_actual_dims:\n",
    "        import math\n",
    "        for i in range(len(points)):\n",
    "            b = points[i]\n",
    "            x0 = b[0][0]\n",
    "            y0 = b[0][1]\n",
    "            x1 = b[1][0]\n",
    "            y1 = b[1][1]\n",
    "            x2 = b[2][0]\n",
    "            y2 = b[2][1]\n",
    "            u0 = -(x0) * x_fac + size[0]\n",
    "            v0 = -(y0 + 40) * y_fac + size[1]\n",
    "            u1 = -(x1) * x_fac + size[0]\n",
    "            v1 = -(y1 + 40) * y_fac + size[1]\n",
    "            u2 = -(x2) * x_fac + size[0]\n",
    "            v2 = -(y2 + 40) * y_fac + size[1]\n",
    "            dimension_length[i] = math.sqrt((v1-v2)**2 + (u1-u2)**2)\n",
    "            dimension_width[i] = math.sqrt((v1-v0)**2 + (u1-u0)**2)\n",
    "            dimension_height[i] = math.sqrt((-(b[0][2]+(-1*z_range[1]))*z_fac-(-b[4][2]+z_range[1])*z_fac)**2)\n",
    "\n",
    "      \n",
    "    for i in range(len(locations)):\n",
    "        if angles[i] < 0:\n",
    "            angles[i] += 3.14\n",
    "\n",
    "    x_range = (x_range[0] + translate_x, x_range[1] + translate_x)\n",
    "    y_range = (y_range[0] + translate_y, y_range[1] + translate_y)\n",
    "    z_range = (z_range[0] + translate_z, z_range[1] + translate_z)\n",
    "    output = [[-(locations[i][0] + -1*x_range[0]) * x_fac + size[0], -(locations[i][1] + -1*y_range[0]) * y_fac + size[1], -(locations[i][2] + -1*z_range[0]) * z_fac + size[2], \n",
    "                dimension_length[i], dimension_width[i], dimension_height[i], angles[i]] \n",
    "                for i in range(len(locations))]\n",
    "    # import math\n",
    "    if ang != 0:\n",
    "        for i in range(len(locations)):\n",
    "            w = size[0]\n",
    "            h = size[1]\n",
    "            output[i][0], output[i][1] = rotate2((w//2, h//2), (output[i][0], output[i][1]), ang / 57.2958)\n",
    "            output[i][6] = output[i][6] - ang / 57.2958\n",
    "\n",
    "    output = list(filter(lambda point: 0 <= point[0] < size[0] and 0 <= point[1] < size[1] and 0 <= point[2] < size[2] , output))\n",
    "    output = np.array(output)\n",
    "\n",
    "    if from_file:\n",
    "        return points, output, calib_data['Tr_velo_to_cam'], calib_data['R0_rect'], calib_data['P2'], directions\n",
    "    else:\n",
    "        return output, indxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms2(label, scores, max_output_size=100, iou_threshold=0.1, sess=None):\n",
    "    boxes = []\n",
    "\n",
    "    for j in range(0, len(label)):\n",
    "\n",
    "        w = label[j][3]\n",
    "        h = label[j][4] \n",
    "        x = label[j][0]\n",
    "        y = label[j][1]\n",
    "        a = label[j][6]\n",
    "        \n",
    "\n",
    "        polygon = convert5Pointto8Point(y, x, w, h, -a*57.2958)\n",
    "        xs = polygon[0::2]\n",
    "        ys = polygon[1::2]\n",
    "            \n",
    "        boxes.append([xs[0], ys[0], xs[2], ys[2]])\n",
    "\n",
    "    boxes = np.array(boxes)\n",
    "    with tf.Graph().as_default():\n",
    "        selected_indices = tf.image.non_max_suppression(\n",
    "                    boxes, scores, max_output_size=max_output_size, iou_threshold=iou_threshold)\n",
    "        if sess is not None:\n",
    "            selected_indices = sess.run(selected_indices)\n",
    "        else:\n",
    "            with tf.Session() as sess:\n",
    "                selected_indices = sess.run(selected_indices)\n",
    "\n",
    "    return selected_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions(final_output, th, new_file_path, apply_nms=False, sess=None):\n",
    "#     now = datetime.now()\n",
    "#     current_time = now.strftime(\"%H:%M:%S\")\n",
    "#     print(\"Current Time =\", current_time)\n",
    "    converted_points = convert_prediction_into_real_values(final_output[0, :, :, :, :], th=th)\n",
    "    points = get_points(converted_points, base_path + '/data_object_calib/training/calib/'+ current_file + '.txt', th=th)\n",
    "#     print(len(points))\n",
    "    res = '\\n'.join([' '.join([str(l) for l in points[i]]) for i in range(len(points))])\n",
    "#     with tf.device('/device:CPU:0'):\n",
    "#     now = datetime.now()\n",
    "#     current_time = now.strftime(\"%H:%M:%S\")\n",
    "#     print(\"Current Time =\", current_time)\n",
    "    if apply_nms:\n",
    "            labels, indxes = read_label2(res, base_path + '/data_object_calib/training/calib/'+ current_file + '.txt', 0, 0, get_actual_dims=True, from_file=False)\n",
    "#             if len(labels) != len(points):\n",
    "#                 print('not the same', new_file_path)\n",
    "    #             return\n",
    "            points = np.array(points)\n",
    "            \n",
    "            if len(labels) > 0:\n",
    "                points = points[indxes]\n",
    "                selected_idx = nms(labels, np.array([points[i][-1] for i in range(len(points))]), max_output_size=100, iou_threshold=0.3, sess=sess)\n",
    "            else:\n",
    "                selected_idx = []\n",
    "\n",
    "            if len(selected_idx) > 0:\n",
    "                points = points[selected_idx]\n",
    "                res = '\\n'.join([' '.join([str(l) for l in points[i]]) for i in range(len(points))])\n",
    "            else:\n",
    "                res=\"\"\n",
    "#     now = datetime.now()\n",
    "#     current_time = now.strftime(\"%H:%M:%S\")\n",
    "#     print(\"Current Time =\", current_time)\n",
    "    text_file = open(new_file_path, \"wb+\")\n",
    "    text_file.write(res.encode())\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions2(th, new_file_path, old_file_path, apply_nms=False, sess=None):\n",
    "#     now = datetime.now()\n",
    "#     current_time = now.strftime(\"%H:%M:%S\")\n",
    "#     print(\"Current Time =\", current_time)\n",
    "    \n",
    "    res = []\n",
    "    with open(old_file_path) as label_file:\n",
    "            res = label_file.readlines()\n",
    "    res = ''.join(res)\n",
    "    points2 = res.split('\\n')\n",
    "    points = []\n",
    "    for i in range(len(points2)):\n",
    "        temp = []\n",
    "        temp = points2[i].split(' ')\n",
    "        points.append(temp)\n",
    "#     print(points.split('\\n')[0])\n",
    "    \n",
    "#     converted_points = convert_prediction_into_real_values(final_output[0, :, :, :, :], th=th)\n",
    "#     points = get_points(converted_points, base_path + '/data_object_calib/training/calib/'+ current_file + '.txt', th=th)\n",
    "#     print(len(points))\n",
    "#     res = '\\n'.join([' '.join([str(l) for l in points[i]]) for i in range(len(points))])\n",
    "#     with tf.device('/device:CPU:0'):\n",
    "#     now = datetime.now()\n",
    "#     current_time = now.strftime(\"%H:%M:%S\")\n",
    "#     print(\"Current Time =\", current_time)\n",
    "    if apply_nms:\n",
    "            labels, indxes = read_label2(res, base_path + '/data_object_calib/training/calib/'+ current_file + '.txt', 0, 0, get_actual_dims=True, from_file=False)\n",
    "#             if len(labels) != len(points):\n",
    "#                 print('not the same', new_file_path, len(labels), len(points))\n",
    "    #             return\n",
    "            points = np.array(points)\n",
    "            \n",
    "            if len(labels) > 0:\n",
    "                points = points[indxes]\n",
    "                selected_idx = nms2(labels, np.array([points[i][-1] for i in range(len(points))]), max_output_size=100, iou_threshold=0.3, sess=sess)\n",
    "            else:\n",
    "                selected_idx = []\n",
    "\n",
    "            if len(selected_idx) > 0:\n",
    "                points = points[selected_idx]\n",
    "                res = '\\n'.join([' '.join([str(l) for l in points[i]]) for i in range(len(points))])\n",
    "            else:\n",
    "                res=\"\"\n",
    "#     now = datetime.now()\n",
    "#     current_time = now.strftime(\"%H:%M:%S\")\n",
    "#     print(\"Current Time =\", current_time)\n",
    "    text_file = open(new_file_path, \"wb+\")\n",
    "    text_file.write(res.encode())\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'fusion': True\n",
    "# }\n",
    "# model = Model(graph=None, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../../../Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_files = list(map(lambda x: x.split('.')[0], os.listdir(base_path+'/data_object_image_3/training/image_3')))\n",
    "# random.seed(0)\n",
    "# random.shuffle(list_files)\n",
    "# ln = int(len(list_files) * 0.5)\n",
    "# list_files= list_files[ln:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training:\n",
    "                file_name = '/trainsplit.txt'\n",
    "else:\n",
    "                file_name = '/valsplit.txt'\n",
    "with open(base_path + file_name, 'r') as f:\n",
    "                list_file_nums = f.readlines()\n",
    "list_files = ['0'*(6-len(l.strip())) + l.strip() for l in list_file_nums]\n",
    "\n",
    "list_camera_paths = list(map(lambda x: base_path+'/data_object_image_3/training/image_3/' + x + '.png', list_files))\n",
    "list_lidar_paths = list(map(lambda x: base_path+'/data_object_velodyne/training/velodyne/' + x + '.bin', list_files))\n",
    "list_label_paths = list(map(lambda x: base_path + '/data_object_label_2/training/label_2/' + x + '.txt', list_files))\n",
    "list_calib_paths = list(map(lambda x: base_path + '/data_object_calib/training/calib/' + x + '.txt', list_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3682"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'new_train_6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '../prediction_files/new_train_6': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"../prediction_files/\"\"$dir_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r \"../prediction_files/\"\"$dir_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th05_2\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th05_2/data\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th10_2\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th10_2/data\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th20_2\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th20_2/data\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th30_2\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th30_2/data\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th40_2\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th40_2/data\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th50_2\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th50_2/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th05_2\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th05_2/data\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th10_2\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th10_2/data\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th20_2\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th20_2/data\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th30_2\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th30_2/data\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th40_2\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th40_2/data\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th50_2\"\n",
    "# !mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th50_2/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# with model.graph.as_default():\n",
    "            \n",
    "#   config = tf.ConfigProto()\n",
    "#   config.gpu_options.allow_growth = True\n",
    "\n",
    "#   with tf.Session(config=config) as sess:\n",
    "#     model.saver.restore(sess, tf.train.latest_checkpoint('../training_files/tmp/'))\n",
    "\n",
    "#     anchor_values = prepare_anchors()\n",
    "#     anchor_values = np.repeat(anchor_values, 1, axis=0)\n",
    "#     dataset = DetectionDatasetLoader(base_path='../../../Data', training_per=0.5, batch_size=1, random_seed=0, training=False)\n",
    "   \n",
    "#     cls_losses = []\n",
    "#     reg_losses = []\n",
    "#     total_losses = []\n",
    "#     i = 0\n",
    "    \n",
    "#     apply_nms=False\n",
    "    \n",
    "#     try:    \n",
    "#         while True:\n",
    "#             feed_dict = prepare_dataset_feed_dict(model, dataset, True, False, anchor_values, False)\n",
    "\n",
    "#             final_output= sess.run(model.final_output, feed_dict=feed_dict)\n",
    "\n",
    "#             if i < len(list_files):\n",
    "#                 current_file = list_files[i]\n",
    "                \n",
    "#                 th = 0.05\n",
    "#                 new_file_path = '../prediction_files/' + dir_name + '/bev/th05_2/data/' + current_file + '.txt'\n",
    "#                 write_predictions(final_output, th, new_file_path, apply_nms=apply_nms, sess=sess)\n",
    "                \n",
    "#                 th = 0.10\n",
    "#                 new_file_path = '../prediction_files/' + dir_name + '/bev/th10_2/data/' + current_file + '.txt'\n",
    "#                 write_predictions(final_output, th, new_file_path, apply_nms=apply_nms, sess=sess)\n",
    "                \n",
    "#                 th = 0.20\n",
    "#                 new_file_path = '../prediction_files/' + dir_name + '/bev/th20_2/data/' + current_file + '.txt'\n",
    "#                 write_predictions(final_output, th, new_file_path, apply_nms=apply_nms, sess=sess)\n",
    "                \n",
    "#                 th = 0.30\n",
    "#                 new_file_path = '../prediction_files/' + dir_name + '/bev/th30_2/data/' + current_file + '.txt'\n",
    "#                 write_predictions(final_output, th, new_file_path, apply_nms=apply_nms)\n",
    "                \n",
    "#                 th = 0.40\n",
    "#                 new_file_path = '../prediction_files/' + dir_name + '/bev/th40_2/data/' + current_file + '.txt'\n",
    "#                 write_predictions(final_output, th, new_file_path, apply_nms=apply_nms)\n",
    "                \n",
    "#                 th = 0.50\n",
    "#                 new_file_path = '../prediction_files/' + dir_name + '/bev/th50_2/data/' + current_file + '.txt'\n",
    "#                 write_predictions(final_output, th, new_file_path, apply_nms=apply_nms)\n",
    "\n",
    "#             else:\n",
    "#                 break\n",
    "#             i += 1\n",
    "#             if i % 100 == 0:\n",
    "#                 print('i = ', i)\n",
    "# #             break\n",
    "#     except tf.errors.OutOfRangeError:\n",
    "#         pass\n",
    "#     except StopIteration:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  2000\n",
      "i =  2100\n",
      "i =  2200\n",
      "i =  2300\n",
      "i =  2400\n",
      "i =  2500\n",
      "i =  2600\n",
      "i =  2700\n",
      "i =  2800\n",
      "i =  2900\n",
      "i =  3000\n",
      "i =  3100\n",
      "i =  3200\n",
      "i =  3300\n",
      "i =  3400\n",
      "i =  3500\n",
      "i =  3600\n"
     ]
    }
   ],
   "source": [
    "old_dir = dir_name\n",
    "if True:\n",
    "            \n",
    "#   config = tf.ConfigProto()\n",
    "#   config.gpu_options.allow_growth = True\n",
    "\n",
    "#   with tf.Session(config=config) as sess:\n",
    "  if True:\n",
    "    sess = None\n",
    "\n",
    "    cls_losses = []\n",
    "    reg_losses = []\n",
    "    total_losses = []\n",
    "    i = 1951\n",
    "    \n",
    "    apply_nms=True\n",
    "    \n",
    "    try:    \n",
    "        while True:\n",
    "\n",
    "            if i < len(list_files):\n",
    "                current_file = list_files[i]\n",
    "                \n",
    "                th = 0.050\n",
    "                old_file_path = '../prediction_files/' + old_dir + '/bev/th05_2/data/' + current_file + '.txt'\n",
    "                new_file_path = '../prediction_files/' + old_dir + '/nms/th05_2/data/' + current_file + '.txt'\n",
    "                write_predictions2(th, new_file_path, old_file_path, apply_nms=apply_nms, sess=sess)\n",
    "                \n",
    "                th = 0.1\n",
    "                old_file_path = '../prediction_files/' + old_dir + '/bev/th10_2/data/' + current_file + '.txt'\n",
    "                new_file_path = '../prediction_files/' + old_dir + '/nms/th10_2/data/' + current_file + '.txt'\n",
    "                write_predictions2(th, new_file_path, old_file_path, apply_nms=apply_nms, sess=sess)\n",
    "                \n",
    "                th = 0.2\n",
    "                old_file_path = '../prediction_files/' + old_dir + '/bev/th20_2/data/' + current_file + '.txt'\n",
    "                new_file_path = '../prediction_files/' + old_dir + '/nms/th20_2/data/' + current_file + '.txt'\n",
    "                write_predictions2(th, new_file_path, old_file_path, apply_nms=apply_nms, sess=sess)\n",
    "                \n",
    "                th = 0.3\n",
    "                old_file_path = '../prediction_files/' + old_dir + '/bev/th30_2/data/' + current_file + '.txt'\n",
    "                new_file_path = '../prediction_files/' + old_dir + '/nms/th30_2/data/' + current_file + '.txt'\n",
    "                write_predictions2(th, new_file_path, old_file_path, apply_nms=apply_nms, sess=sess)\n",
    "                \n",
    "                th = 0.4\n",
    "                old_file_path = '../prediction_files/' + old_dir + '/bev/th40_2/data/' + current_file + '.txt'\n",
    "                new_file_path = '../prediction_files/' + old_dir + '/nms/th40_2/data/' + current_file + '.txt'\n",
    "                write_predictions2(th, new_file_path, old_file_path, apply_nms=apply_nms, sess=sess)\n",
    "                \n",
    "                th = 0.50\n",
    "                old_file_path = '../prediction_files/' + old_dir + '/bev/th50_2/data/' + current_file + '.txt'\n",
    "                new_file_path = '../prediction_files/' + old_dir + '/nms/th50_2/data/' + current_file + '.txt'\n",
    "                write_predictions2(th, new_file_path, old_file_path, apply_nms=apply_nms, sess=sess)\n",
    "                \n",
    "\n",
    "            else:\n",
    "#                 pass\n",
    "                break\n",
    "            i += 1\n",
    "            if i % 100 == 0:\n",
    "                print('i = ', i)\n",
    "#             break\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"i at break1 = \", i)\n",
    "    except StopIteration:\n",
    "        print(\"i at break2 = \", i)\n",
    "    except:\n",
    "        print(\"i at break3 = \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['001966', '003202', '005344', '001177', '000534', '002694', '004495',\\\n",
    "          '001334', '000611', '007052', '001813', '000195', '004809', '000620',\\\n",
    "          '000803', '004562', '002591', '003033', '001587', '005985', '006109',\\\n",
    "          '004775']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['001966', '003202', '005344', '001177', '000534', '002694', '004495', '001334',\\\n",
    " '000611', '007052', '001813', '000195', '004809', '000620', '000803', '004562',\\\n",
    " '002591', '003033', '005985', '006109', '004775']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
